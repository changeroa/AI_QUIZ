"""
PDF í…ìŠ¤íŠ¸ ë¶„ì„ê¸° - ì•„í‚¤í…ì²˜ ê¸°ë°˜ ê°œì„  ë²„ì „ (ì™„ì „ ì¬ì‘ì„±)
í•µì‹¬ ê¸°ëŠ¥: í˜ì´ì§€ë³„ Gap ë¶„ì„ â†’ Outlier íƒì§€ â†’ ì°¨ë³„ì  ì²˜ë¦¬
"""

import fitz
import json
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional, Tuple
import statistics
from pathlib import Path
import requests

# í•œê¸€ í°íŠ¸ ì„¤ì •
import platform
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

def setup_korean_fonts():
    """ì‹œìŠ¤í…œì— ë§ëŠ” í•œê¸€ í°íŠ¸ ì„¤ì •"""
    system = platform.system()
    if system == 'Windows':
        plt.rcParams['font.family'] = ['Malgun Gothic', 'Arial Unicode MS']
    elif system == 'Darwin':
        plt.rcParams['font.family'] = ['AppleGothic', 'Arial Unicode MS']
    else:
        plt.rcParams['font.family'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

setup_korean_fonts()


class EnhancedTextClassifier:
    """ì•„í‚¤í…ì²˜ ê¸°ë°˜ ê°œì„ ëœ í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, hf_token: str = None, verbose: bool = True):
        self.verbose = verbose
        self.hf_token = hf_token
        
    def analyze_pdf_enhanced(self, pdf_path: str) -> Dict[str, Any]:
        """ì•„í‚¤í…ì²˜ ê¸°ë°˜ PDF ë¶„ì„"""
        if self.verbose:
            print(f"ğŸ” Enhanced PDF ë¶„ì„: {pdf_path}")
        
        # ê¸°ì¡´ analyze_font_sizes ë©”ì„œë“œ ì‚¬ìš©
        analysis = self._analyze_font_sizes_original(pdf_path)
        
        # Phase 1: í˜ì´ì§€ë³„ Gap íŒ¨í„´ ë¶„ì„
        page_patterns = self._analyze_page_gap_patterns(analysis["page_elements"])
        
        # Phase 2: ì „ì²´ Gap íŒ¨í„´ ë¶„ì„
        overall_pattern = self._analyze_overall_gap_patterns(page_patterns)
        
        # Phase 3: Outlier íƒì§€ - ì´ì œ í™•ì‹¤íˆ ì‘ë™í•¨
        outlier_result = self._detect_outlier_pages(page_patterns, overall_pattern)
        normal_pages = outlier_result["normal_pages"]
        anomaly_pages = outlier_result["anomaly_pages"]
        
        if self.verbose:
            print(f"ğŸ“Š ì¼ë°˜: {len(normal_pages)}í˜ì´ì§€, ì´ìƒì¹˜: {len(anomaly_pages)}í˜ì´ì§€")
        
        return {
            "analysis": analysis,
            "page_patterns": page_patterns,
            "overall_pattern": overall_pattern,
            "normal_pages": normal_pages,
            "anomaly_pages": anomaly_pages
        }
    
    def _analyze_font_sizes_original(self, pdf_path: str) -> Dict[str, Any]:
        """ê¸°ì¡´ analyze_font_sizes ë©”ì„œë“œ"""
        if self.verbose:
            print(f"ğŸ“– PDF ë¶„ì„ ì¤‘: {pdf_path}")
        
        try:
            doc = fitz.open(pdf_path)
            
            size_frequency = Counter()
            all_elements = []
            page_elements = defaultdict(list)
            
            try:
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    text_dict = page.get_text("dict")
                    
                    for block in text_dict["blocks"]:
                        if "lines" not in block:
                            continue
                            
                        for line in block["lines"]:
                            for span in line["spans"]:
                                text = span["text"].strip()
                                if not text:
                                    continue
                                
                                size = round(span["size"], 1)
                                size_frequency[size] += 1
                                
                                element = {
                                    "page": page_num + 1,
                                    "text": text,
                                    "size": size,
                                    "font": span["font"],
                                    "bbox": span["bbox"],
                                    "flags": span.get("flags", 0),
                                    "color": span.get("color", 0)
                                }
                                all_elements.append(element)
                                page_elements[page_num + 1].append(element)
            finally:
                doc.close()
                
        except Exception as e:
            print(f"âŒ PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        
        # í†µê³„ ê³„ì‚°
        sizes = list(size_frequency.keys())
        if not sizes:
            return {
                "size_frequency": {},
                "all_elements": [],
                "page_elements": {},
                "statistics": {
                    "min_size": 0, "max_size": 0, "mean_size": 0,
                    "median_size": 0, "unique_sizes": 0, "total_texts": 0, "total_pages": 0
                }
            }
            
        sizes.sort()
        
        weighted_sum = sum(size * count for size, count in size_frequency.items())
        total_count = sum(size_frequency.values())
        weighted_mean = weighted_sum / total_count if total_count > 0 else 0
        
        analysis = {
            "size_frequency": dict(size_frequency),
            "all_elements": all_elements,
            "page_elements": dict(page_elements),
            "statistics": {
                "min_size": min(sizes),
                "max_size": max(sizes),
                "mean_size": weighted_mean,
                "median_size": statistics.median(sizes),
                "unique_sizes": len(sizes),
                "total_texts": total_count,
                "total_pages": len(page_elements)
            }
        }
        
        if self.verbose:
            print(f"âœ… ë¶„ì„ ì™„ë£Œ: {total_count}ê°œ í…ìŠ¤íŠ¸, {len(sizes)}ê°œ ê³ ìœ  í¬ê¸°")
        
        return analysis
    
    def _analyze_page_gap_patterns(self, page_elements: Dict) -> List[Dict]:
        """í˜ì´ì§€ë³„ Gap íŒ¨í„´ ë¶„ì„"""
        page_patterns = []
        
        for page_num, elements in page_elements.items():
            if not elements:
                continue
                
            sizes = [elem["size"] for elem in elements]
            unique_sizes = sorted(set(sizes))
            
            # Gap ê³„ì‚°
            gaps = []
            if len(unique_sizes) > 1:
                gaps = [unique_sizes[i+1] - unique_sizes[i] for i in range(len(unique_sizes)-1)]
            
            pattern = {
                "page_number": page_num,
                "unique_sizes": unique_sizes,
                "gaps": gaps,
                "largest_gap": max(gaps) if gaps else 0,
                "hierarchy_levels": len(unique_sizes),
                "total_elements": len(elements),
                "size_range": max(sizes) - min(sizes) if sizes else 0,
                "min_size": min(sizes) if sizes else 0,
                "max_size": max(sizes) if sizes else 0
            }
            
            page_patterns.append(pattern)
        
        if self.verbose:
            print(f"ğŸ“„ í˜ì´ì§€ë³„ Gap íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: {len(page_patterns)}ê°œ í˜ì´ì§€")
        
        return page_patterns
    
    def _analyze_overall_gap_patterns(self, page_patterns: List[Dict]) -> Dict[str, Any]:
        """ì „ì²´ Gap íŒ¨í„´ ë¶„ì„"""
        if not page_patterns:
            return {}
        
        # ì „ì²´ í†µê³„
        all_gaps = []
        all_hierarchy_levels = []
        all_size_ranges = []
        all_max_sizes = []
        
        for pattern in page_patterns:
            all_gaps.extend(pattern["gaps"])
            all_hierarchy_levels.append(pattern["hierarchy_levels"])
            all_size_ranges.append(pattern["size_range"])
            all_max_sizes.append(pattern["max_size"])
        
        overall_pattern = {
            "median_gap": statistics.median(all_gaps) if all_gaps else 0,
            "mean_gap": statistics.mean(all_gaps) if all_gaps else 0,
            "median_hierarchy": statistics.median(all_hierarchy_levels),
            "median_size_range": statistics.median(all_size_ranges),
            "median_max_size": statistics.median(all_max_sizes),
            "total_pages": len(page_patterns)
        }
        
        if self.verbose:
            print(f"ğŸŒ ì „ì²´ Gap íŒ¨í„´: ì¤‘ê°„ê°’ Gap {overall_pattern['median_gap']:.1f}pt")
        
        return overall_pattern
    
    def _detect_outlier_pages(self, page_patterns: List[Dict], overall_pattern: Dict) -> Dict[str, List[Dict]]:
        """Outlier í˜ì´ì§€ íƒì§€ - ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ"""
        normal_pages = []
        anomaly_pages = []
        
        for pattern in page_patterns:
            anomaly_reasons = []
            
            # ì¡°ê±´ 1: ë¹„ì •ìƒì ìœ¼ë¡œ í° Gap
            if overall_pattern.get("median_gap", 0) > 0 and pattern["largest_gap"] > overall_pattern["median_gap"] * 2.5:
                anomaly_reasons.append("large_gap")
            
            # ì¡°ê±´ 2: ë„ˆë¬´ ë§ì€ ê³„ì¸µ
            if pattern["hierarchy_levels"] > overall_pattern.get("median_hierarchy", 5) * 2:
                anomaly_reasons.append("too_many_levels")
            
            # ì¡°ê±´ 3: í¬ê¸° ë²”ìœ„ê°€ ë„ˆë¬´ ë„“ìŒ
            if pattern["size_range"] > overall_pattern.get("median_size_range", 10) * 2:
                anomaly_reasons.append("wide_size_range")
            
            # ì¡°ê±´ 4: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìŒ (ì œëª© í˜ì´ì§€ ë“±)
            if pattern["total_elements"] < 5:
                anomaly_reasons.append("few_elements")
            
            # ì¡°ê±´ 5: ë¹„ì •ìƒì ìœ¼ë¡œ í° í°íŠ¸
            if pattern["max_size"] > overall_pattern.get("median_max_size", 12) * 1.5:
                anomaly_reasons.append("large_font")
            
            # 2ê°œ ì´ìƒ ì¡°ê±´ ì¶©ì¡±ì‹œ ì´ìƒì¹˜
            if len(anomaly_reasons) >= 2:
                pattern["anomaly_reasons"] = anomaly_reasons
                anomaly_pages.append(pattern)
                if self.verbose:
                    print(f"ğŸ“„ ì´ìƒì¹˜ í˜ì´ì§€ {pattern['page_number']}: {', '.join(anomaly_reasons)}")
            else:
                normal_pages.append(pattern)
        
        if self.verbose:
            print(f"ğŸ“Š Outlier íƒì§€ ì™„ë£Œ: ì¼ë°˜ {len(normal_pages)}ê°œ, ì´ìƒì¹˜ {len(anomaly_pages)}ê°œ")
        
        return {
            "normal_pages": normal_pages,
            "anomaly_pages": anomaly_pages
        }
    
    def classify_with_new_hybrid_approach(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ìœ¼ë¡œ ë¶„ë¥˜"""
        if self.verbose:
            print("ğŸ¯ ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ ì‹œì‘...")
        
        normal_pages = analysis_result["normal_pages"]
        anomaly_pages = analysis_result["anomaly_pages"]
        all_elements = analysis_result["analysis"]["all_elements"]
        
        # Phase 4: ì¼ë°˜ í˜ì´ì§€ ì„ê³„ê°’ ê³„ì‚° (ì¤‘ê°„ 80% + K-means)
        normal_elements = self._get_normal_page_elements(normal_pages, all_elements)
        thresholds = self._calculate_robust_thresholds(normal_elements)
        
        # Phase 5: ë¶„ë¥˜ ìˆ˜í–‰
        classified_results = {
            "normal_classified": [],
            "anomaly_classified": [],
            "classification_stats": Counter(),
            "thresholds": thresholds
        }
        
        # ì¼ë°˜ í˜ì´ì§€ ë¶„ë¥˜ (í†µê³„ì  ë°©ë²•)
        for page_pattern in normal_pages:
            page_num = page_pattern["page_number"]
            page_elements = [e for e in all_elements if e["page"] == page_num]
            
            classified_elements = self._classify_with_statistical_method(page_elements, thresholds)
            classified_results["normal_classified"].extend(classified_elements)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            for elem in classified_elements:
                classified_results["classification_stats"][elem["classification"]] += 1
        
        # ì´ìƒì¹˜ í˜ì´ì§€ ì²˜ë¦¬
        for page_pattern in anomaly_pages:
            page_num = page_pattern["page_number"]
            page_elements = [e for e in all_elements if e["page"] == page_num]
            
            if self.hf_token:
                # LLM ì²˜ë¦¬ ì‹œë„
                try:
                    classified_elements = self._process_with_llm(page_elements, page_pattern.get("anomaly_reasons", []))
                    classified_results["anomaly_classified"].extend(classified_elements)
                    if self.verbose:
                        print(f"âœ… í˜ì´ì§€ {page_num} LLM ì²˜ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    if self.verbose:
                        print(f"âš ï¸ í˜ì´ì§€ {page_num} LLM ì‹¤íŒ¨, ëŒ€ì•ˆ ì‚¬ìš©: {e}")
                    classified_elements = self._classify_anomaly_fallback(page_elements)
                    classified_results["anomaly_classified"].extend(classified_elements)
            else:
                # LLM ì—†ìœ¼ë©´ ëŒ€ì•ˆ ë¶„ë¥˜
                classified_elements = self._classify_anomaly_fallback(page_elements)
                classified_results["anomaly_classified"].extend(classified_elements)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            for elem in classified_elements:
                classified_results["classification_stats"][elem["classification"]] += 1
        
        # ì „ì²´ ê²°ê³¼ í†µí•©
        all_classified = (classified_results["normal_classified"] + 
                         classified_results["anomaly_classified"])
        classified_results["classified_elements"] = all_classified
        
        if self.verbose:
            print(f"âœ… ë¶„ë¥˜ ì™„ë£Œ: ì´ {len(all_classified)}ê°œ ìš”ì†Œ")
        
        return classified_results
    
    def _get_normal_page_elements(self, normal_pages: List[Dict], all_elements: List[Dict]) -> List[Dict]:
        """ì¼ë°˜ í˜ì´ì§€ì˜ ëª¨ë“  ìš”ì†Œ ìˆ˜ì§‘"""
        normal_page_nums = [p["page_number"] for p in normal_pages]
        return [e for e in all_elements if e["page"] in normal_page_nums]
    
    def _calculate_robust_thresholds(self, elements: List[Dict]) -> Dict[str, float]:
        """ê²¬ê³ í•œ ì„ê³„ê°’ ê³„ì‚°: ì¤‘ê°„ 80% + K-means ì¡°í•©"""
        if not elements:
            return {"title_threshold": 18.0, "subtitle_threshold": 14.0, "body_threshold": 12.0}
        
        sizes = [e["size"] for e in elements]
        
        # ë°©ë²• 1: ì¤‘ê°„ 80% ë²”ìœ„ (ì¢Œìš° 10%ì”© ì œì™¸)
        middle_80_thresholds = self._calculate_middle_80_thresholds(sizes)
        
        # ë°©ë²• 2: K-means í´ëŸ¬ìŠ¤í„°ë§ (k=3)
        kmeans_thresholds = self._calculate_kmeans_thresholds(sizes)
        
        # ë°©ë²•3 : yì¢Œí‘œê¸°ë°˜ ì„ê³„ê°’ê³„ì‚°
        y_thresholds = self._calculate_y_position_thresholds(elements)
        # ê°€ì¤‘ ì¡°í•©
        final_thresholds = {
            "title_threshold": (middle_80_thresholds["title"] * 0.7 + 
                              kmeans_thresholds["title"] * 0.3),
            "subtitle_threshold": (middle_80_thresholds["subtitle"] * 0.7 + 
                                 kmeans_thresholds["subtitle"] * 0.3),
            "body_threshold": (middle_80_thresholds["body"] * 0.7 + 
                             kmeans_thresholds["body"] * 0.3),
            "title_y": y_thresholds["title"],
            
            "subtitle_y": y_thresholds["subtitle"],

            "body_y": y_thresholds["body"]
        }
        
        if self.verbose:
            print(f"ğŸ“ ì„ê³„ê°’: Titleâ‰¥{final_thresholds['title_threshold']:.1f}, "
                  f"Subtitleâ‰¥{final_thresholds['subtitle_threshold']:.1f}, "
                  f"Bodyâ‰¥{final_thresholds['body_threshold']:.1f}")
        
        return final_thresholds
    
    def _calculate_y_position_thresholds(self, elements: List[Dict]) -> Dict[str, float]:
        """
        y ì¢Œí‘œ ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°: ë¸”ë¡ ê°„ ìƒëŒ€ ê±°ë¦¬ ê¸°ë°˜
        y ê°’ì´ ì‘ì„ìˆ˜ë¡ í˜ì´ì§€ ìƒë‹¨ì— ìœ„ì¹˜ (PDF ì¢Œí‘œê³„ ê¸°ì¤€)
        """
        DEFAULT_TITLE_Y = 200.0
        DEFAULT_SUBTITLE_Y = 400.0
        PAGE_HEIGHT = 842.0  # A4 ê¸°ì¤€
        
        if not elements:
            return {"title": DEFAULT_TITLE_Y, "subtitle": DEFAULT_SUBTITLE_Y, "body": PAGE_HEIGHT}

        try:
            # y ê°’ì´ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ í•„í„°ë§
            y_values = sorted(set(e.get("y") for e in elements if isinstance(e.get("y"), (int, float))))
            if len(y_values) < 3:
                return {"title": DEFAULT_TITLE_Y, "subtitle": DEFAULT_SUBTITLE_Y, "body": PAGE_HEIGHT}

            gaps = [(i, y_values[i+1] - y_values[i]) for i in range(len(y_values) - 1)]
            sorted_gaps = sorted(gaps, key=lambda x: x[1], reverse=True)
            top_two = sorted(sorted_gaps[:2], key=lambda x: x[0])

            if len(top_two) < 2:
                return {"title": DEFAULT_TITLE_Y, "subtitle": DEFAULT_SUBTITLE_Y, "body": PAGE_HEIGHT}

            idx1, _ = top_two[0]
            idx2, _ = top_two[1]

            title_y = y_values[idx1]
            subtitle_y = y_values[idx2]
            body_y = PAGE_HEIGHT

            if getattr(self, 'verbose', False):
                print(f" yì¢Œí‘œ ê¸°ë°˜ ì„ê³„ê°’: title â‰¤ {title_y:.1f}, subtitle â‰¤ {subtitle_y:.1f}, body â‰¤ {body_y:.1f}")

            return {
                "title": title_y,
                "subtitle": subtitle_y,
                "body": body_y
            }

        except Exception as e:
            if getattr(self, 'verbose', False):
                print(f" y ì¢Œí‘œ ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"title": DEFAULT_TITLE_Y, "subtitle": DEFAULT_SUBTITLE_Y, "body": PAGE_HEIGHT}
        

    def _calculate_middle_80_thresholds(self, sizes: List[float]) -> Dict[str, float]:
        """ì¤‘ê°„ 80% ë²”ìœ„ ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°"""
        sorted_sizes = sorted(sizes)
        n = len(sorted_sizes)
        
        # ì¢Œìš° 10%ì”© ì œì™¸í•œ ì¤‘ê°„ 80% ë²”ìœ„
        start_idx = int(n * 0.1)
        end_idx = int(n * 0.9)
        middle_80_sizes = sorted_sizes[start_idx:end_idx]
        
        if not middle_80_sizes:
            return {"title": 18.0, "subtitle": 14.0, "body": 12.0}
        
        # ì¤‘ê°„ ë²”ìœ„ë¥¼ 3ë“±ë¶„
        third = len(middle_80_sizes) // 3
        
        return {
            "body": middle_80_sizes[third] if third < len(middle_80_sizes) else middle_80_sizes[0],
            "subtitle": middle_80_sizes[2*third] if 2*third < len(middle_80_sizes) else middle_80_sizes[-1],
            "title": middle_80_sizes[-1]
        }
    
    def _calculate_kmeans_thresholds(self, sizes: List[float]) -> Dict[str, float]:
        """K-means í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°"""
        try:
            from sklearn.cluster import KMeans
            import numpy as np
            
            # K-means k=3ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
            sizes_array = np.array(sizes).reshape(-1, 1)
            kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(sizes_array)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° ë¶„í¬
            cluster_sizes = {}
            for i in range(3):
                cluster_elements = [sizes[j] for j in range(len(sizes)) if clusters[j] == i]
                if cluster_elements:
                    cluster_sizes[i] = {
                        "min": min(cluster_elements),
                        "max": max(cluster_elements),  
                        "count": len(cluster_elements),
                        "center": kmeans.cluster_centers_[i][0]
                    }
            
            if not cluster_sizes:
                return {"title": 18.0, "subtitle": 14.0, "body": 12.0}
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_clusters = sorted(cluster_sizes.keys(), 
                                   key=lambda k: cluster_sizes[k]["center"])
            
            # ì„ê³„ê°’ ì„¤ì •
            if len(sorted_clusters) >= 3:
                return {
                    "body": cluster_sizes[sorted_clusters[0]]["max"],
                    "subtitle": cluster_sizes[sorted_clusters[1]]["max"],
                    "title": cluster_sizes[sorted_clusters[2]]["center"]
                }
            else:
                return {"title": 18.0, "subtitle": 14.0, "body": 12.0}
                
        except ImportError:
            if self.verbose:
                print("âš ï¸ sklearnì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return {"title": 18.0, "subtitle": 14.0, "body": 12.0}
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ K-means ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            return {"title": 18.0, "subtitle": 14.0, "body": 12.0}
    
    def _classify_with_statistical_method(self, elements: List[Dict], thresholds: Dict) -> List[Dict]:
        """í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ë¶„ë¥˜ (ì¼ë°˜ í˜ì´ì§€ìš©)"""
        classified = []
        
        for elem in elements:
            elem_copy = elem.copy()
            size = elem["size"]
            y = elem.get("y", 9999.0)
            text = elem["text"].strip()
            
            # íŒ¨í„´ ê¸°ë°˜ ìš°ì„  ë¶„ë¥˜
            if len(text) <= 3 and text.isdigit():
                classification = "page_number"
            elif size >= thresholds["title_threshold"] and y <= thresholds.get("title_y", -1):
                classification = "title"
            elif size >= thresholds["subtitle_threshold"] and y <= thresholds.get("subtitle_y", -1):
                classification = "subtitle"  
            elif text.startswith(('â€¢', '-', '*', '1.', '2.', '3.')):
                classification = "bullet_point"
            elif size >= thresholds["body_threshold"]:
                classification = "body_text"
            else:
                classification = "footnote"
            
            elem_copy["classification"] = classification
            elem_copy["method"] = "statistical"
            classified.append(elem_copy)
        
        return classified
    
    def _process_with_llm(self, elements: List[Dict], anomaly_reasons: List[str]) -> List[Dict]:
        """LLMìœ¼ë¡œ ì´ìƒì¹˜ í˜ì´ì§€ ì²˜ë¦¬"""
        # ê°„ë‹¨í•œ LLM ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” Hugging Face API í˜¸ì¶œ)
        classified = []
        
        for elem in elements:
            elem_copy = elem.copy()
            size = elem["size"]
            text = elem["text"]
            
            # LLM ê¸°ë°˜ ê³ ê¸‰ ë¶„ë¥˜ (ì‹œë®¬ë ˆì´ì…˜)
            if "few_elements" in anomaly_reasons and size > 14:
                classification = "title"
            elif any(keyword in text.lower() for keyword in ['chapter', 'section', 'ì¥', 'ì ˆ']):
                classification = "structured_title"
            elif size > 16:
                classification = "title"
            elif size > 12:
                classification = "subtitle"
            else:
                classification = "body_text"
            
            elem_copy["classification"] = classification
            elem_copy["method"] = "llm_simulation"
            classified.append(elem_copy)
        
        return classified
    
    def _classify_anomaly_fallback(self, elements: List[Dict]) -> List[Dict]:
        """ì´ìƒì¹˜ í˜ì´ì§€ ëŒ€ì•ˆ ë¶„ë¥˜"""
        classified = []
        
        for elem in elements:
            elem_copy = elem.copy()
            size = elem["size"]
            
            # ë‹¨ìˆœí•œ í¬ê¸° ê¸°ë°˜ ë¶„ë¥˜
            if size > 18:
                classification = "title"
            elif size > 14:
                classification = "subtitle"
            elif size > 10:
                classification = "body_text"
            else:
                classification = "footnote"
            
            elem_copy["classification"] = classification
            elem_copy["method"] = "fallback"
            classified.append(elem_copy)
        
        return classified


def analyze_lecture_pdf_new_hybrid(pdf_path: str, 
                                  huggingface_token: str = None,
                                  output_dir: str = None,
                                  show_visualizations: bool = True) -> Dict[str, Any]:
    """ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ New Hybrid Approach Analysis")
    print("="*40)
    
    # ë¶„ì„ ìˆ˜í–‰
    classifier = EnhancedTextClassifier(huggingface_token, verbose=True)
    
    # Phase 1-3: í˜ì´ì§€ ë¶„ì„, Gap íŒ¨í„´, Outlier íƒì§€
    analysis_result = classifier.analyze_pdf_enhanced(pdf_path)
    
    # Phase 4-5: ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜
    classification_result = classifier.classify_with_new_hybrid_approach(analysis_result)
    
    # ê²°ê³¼ í†µí•©
    results = {
        "analysis": analysis_result,
        "classification": classification_result,
        "pdf_path": pdf_path
    }
    
    # ìš”ì•½ ì¶œë ¥
    print_new_hybrid_summary(results)
    
    # ê²°ê³¼ ì €ì¥
    if output_dir:
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        with open(f"{output_dir}/new_hybrid_results.json", "w", encoding="utf-8") as f:
            save_data = {
                "statistics": analysis_result["analysis"]["statistics"],
                "normal_pages": len(analysis_result["normal_pages"]),
                "anomaly_pages": len(analysis_result["anomaly_pages"]),
                "anomaly_reasons": [p.get("anomaly_reasons", []) for p in analysis_result["anomaly_pages"]],
                "classified_elements": classification_result["classified_elements"],
                "classification_stats": dict(classification_result["classification_stats"]),
                "thresholds": classification_result["thresholds"]
            }
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_dir}")
    
    return results


def print_new_hybrid_summary(results: Dict[str, Any]):
    """ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    analysis = results["analysis"]
    classification = results["classification"]
    
    print("\n" + "="*60)
    print("ğŸ“ˆ NEW HYBRID APPROACH SUMMARY")
    print("="*60)
    
    stats = analysis["analysis"]["statistics"]
    print(f"ğŸ“„ ì´ í˜ì´ì§€: {stats['total_pages']}")
    print(f"ğŸ“ ì´ í…ìŠ¤íŠ¸: {stats['total_texts']:,}ê°œ")
    print(f"ğŸ“ í¬ê¸° ë²”ìœ„: {stats['min_size']:.1f}pt ~ {stats['max_size']:.1f}pt")
    
    print(f"\nğŸ” í˜ì´ì§€ ë¶„ë¥˜:")
    print(f"  â€¢ ì¼ë°˜ í˜ì´ì§€: {len(analysis['normal_pages'])}ê°œ (í†µê³„ì  ë°©ë²•)")
    print(f"  â€¢ ì´ìƒì¹˜ í˜ì´ì§€: {len(analysis['anomaly_pages'])}ê°œ (LLM ì²˜ë¦¬)")
    
    # ì´ìƒì¹˜ ì›ì¸
    if analysis["anomaly_pages"]:
        all_reasons = []
        for page in analysis["anomaly_pages"]:
            all_reasons.extend(page.get("anomaly_reasons", []))
        reason_counts = Counter(all_reasons)
        
        print(f"\nâš ï¸ ì´ìƒì¹˜ ì›ì¸:")
        for reason, count in reason_counts.most_common():
            print(f"  â€¢ {reason}: {count}íšŒ")
    
    print(f"\nğŸ·ï¸ í…ìŠ¤íŠ¸ ë¶„ë¥˜:")
    for category, count in classification["classification_stats"].most_common():
        print(f"  â€¢ {category}: {count:,}ê°œ")
    
    thresholds = classification["thresholds"]
    print(f"\nğŸ“ ê³„ì‚°ëœ ì„ê³„ê°’:")
    print(f"  â€¢ Title: â‰¥{thresholds['title_threshold']:.1f}pt")
    print(f"  â€¢ Subtitle: â‰¥{thresholds['subtitle_threshold']:.1f}pt")
    print(f"  â€¢ Body: â‰¥{thresholds['body_threshold']:.1f}pt")
    
    # ì²˜ë¦¬ ë°©ë²•ë³„ í†µê³„
    methods = Counter()
    for elem in classification["classified_elements"]:
        methods[elem.get("method", "unknown")] += 1
    
    print(f"\nğŸ”§ ì²˜ë¦¬ ë°©ë²•ë³„ í†µê³„:")
    for method, count in methods.most_common():
        print(f"  â€¢ {method}: {count:,}ê°œ")


if __name__ == "__main__":
    import sys
    
    # ì‚¬ìš©ë²•: python text_analyzer.py [PDFê²½ë¡œ] [--hf-token TOKEN]
    pdf_path = sys.argv[1] if len(sys.argv) > 1 else "sample.pdf"
    hf_token = None
    
    if "--hf-token" in sys.argv:
        try:
            idx = sys.argv.index("--hf-token") + 1
            hf_token = sys.argv[idx]
            print("ğŸ”‘ Hugging Face í† í° ì„¤ì •ë¨")
        except:
            print("âš ï¸ Hugging Face í† í° í˜•ì‹ ì˜¤ë¥˜")
    
    try:
        results = analyze_lecture_pdf_new_hybrid(
            pdf_path=pdf_path,
            huggingface_token=hf_token,
            output_dir="./new_hybrid_output",
            show_visualizations=True
        )
        print("\nâœ… ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()